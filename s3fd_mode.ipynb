{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3FD Face Recognition\n",
    "\n",
    "This is the file that runs S3FD for the face detection model. The model is implemented with Caffe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load model\n",
    "prototxt_path = \"../ssic_image-corpus/scripts/benchmark/sfd_models/models/VGGNet/WIDER_FACE/SFD_trained/deploy.prototxt\"\n",
    "caffemodel_path = \"../ssic_image-corpus/scripts/benchmark/sfd_models/models/VGGNet/WIDER_FACE/SFD_trained/SFD.caffemodel\"\n",
    "net = cv2.dnn.readNetFromCaffe(prototxt_path, caffemodel_path)\n",
    "\n",
    "\n",
    "image = cv2.imread(\"sample_image.png\")                          # Read image (BGR)\n",
    "(h, w) = image.shape[:2]                                # Original dimensions\n",
    "blob = cv2.dnn.blobFromImage(\n",
    "    image, \n",
    "    scalefactor=1.0,                     # No scaling (pixel range [0,255])\n",
    "    size=(640, 640),                     # Input size defined in .prototxt\n",
    "    mean=(104.0, 177.0, 123.0),         # Mean subtraction (BGR)\n",
    "    swapRB=False,                        # OpenCV loads as BGR, no swap needed\n",
    "    crop=False                           # Don't crop (resize while preserving aspect ratio)\n",
    ")\n",
    "\n",
    "\n",
    "net.setInput(blob)\n",
    "detections = net.forward()          \n",
    "\n",
    "\n",
    "for i in range(detections.shape[2]):\n",
    "    confidence = detections[0, 0, i, 2]  # Confidence score\n",
    "    if confidence > 0.5:                \n",
    "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])  # Scale to original image\n",
    "        (x1, y1, x2, y2) = box.astype(\"int\")\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2) # Draw box\n",
    "\n",
    "\n",
    "cv2.imshow(\"Output\", image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.         1.         0.99896395 ... 0.15981525 0.7683409  0.35251033]\n",
      "  [0.         1.         0.05801576 ... 0.52500653 0.08713106 0.6077765 ]\n",
      "  [0.         1.         0.05171308 ... 0.21051191 0.3296745  0.2721469 ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "for det in detections:\n",
    "    print(det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_to_yolo_format(detections, image_width, image_height, class_id=0, outputPath=None):\n",
    "\n",
    "    yolo_detections = []\n",
    "    \n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.5:  # Apply confidence threshold\n",
    "            # Get normalized box coordinates (x1, y1, x2, y2)\n",
    "            box = detections[0, 0, i, 3:7]\n",
    "            \n",
    "            # Convert to (x_center, y_center, width, height) and normalize\n",
    "            x_center = (box[0] + box[2]) / 2.0  # (x1 + x2)/2\n",
    "            y_center = (box[1] + box[3]) / 2.0  # (y1 + y2)/2\n",
    "            width = box[2] - box[0]             # x2 - x1\n",
    "            height = box[3] - box[1]            # y2 - y1\n",
    "            \n",
    "            # Append YOLO-format detection\n",
    "            yolo_detections.append([\n",
    "                class_id,\n",
    "                x_center,      \n",
    "                y_center,      \n",
    "                height,        \n",
    "            ])\n",
    "    \n",
    "    # Save YOLO-format detections to file\n",
    "    # with open(outputPath, \"w\") as file:\n",
    "    #     for detection in yolo_detections:\n",
    "    #         file.write(\" \".join([str(x) for x in detection]) + \"\\n\")\n",
    "\n",
    "    return yolo_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0.7115015983581543, 0.2561627924442291, 0.113678575, 0.19269508, 0.99896395]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "yolo_results = convert_to_yolo_format(detections, image.shape[1], image.shape[0])\n",
    "\n",
    "# Example output for one face:\n",
    "# [[0, 0.45, 0.6, 0.1, 0.15, 0.98]]  # [class, x_center, y_center, w, h, conf]\n",
    "\n",
    "print(yolo_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO-to-BBox: (314, 57, 368, 126)\n"
     ]
    }
   ],
   "source": [
    "def yolo_to_bbox(yolo_det, img_width, img_height):\n",
    "\n",
    "    x_center, y_center, w, h = yolo_det[1], yolo_det[2], yolo_det[3], yolo_det[4]\n",
    "    x1 = int((x_center - w / 2) * img_width)\n",
    "    y1 = int((y_center - h / 2) * img_height)\n",
    "    x2 = int((x_center + w / 2) * img_width)\n",
    "    y2 = int((y_center + h / 2) * img_height)\n",
    "    return (x1, y1, x2, y2)\n",
    "\n",
    "# Check if the converted box matches OpenCV's original detection\n",
    "for det in yolo_results:\n",
    "    bbox = yolo_to_bbox(det, image.shape[1], image.shape[0])\n",
    "    print(\"YOLO-to-BBox:\", bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def yolo_to_bbox_and_plot(yolo_detections, image, color=(0, 255, 255), thickness=2):\n",
    "\n",
    "    img_height, img_width = image.shape[:2]\n",
    "    output_image = image.copy()\n",
    "    \n",
    "    for det in yolo_detections:\n",
    "        class_id, x_center, y_center, w, h, confidence = det\n",
    "        \n",
    "        # Convert YOLO to pixel coordinates\n",
    "        x1 = int((x_center - w / 2) * img_width)\n",
    "        y1 = int((y_center - h / 2) * img_height)\n",
    "        x2 = int((x_center + w / 2) * img_width)\n",
    "        y2 = int((y_center + h / 2) * img_height)\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(output_image, (x1, y1), (x2, y2), color, thickness)\n",
    "        \n",
    "        # Label with confidence\n",
    "        label = f\"Face: {confidence:.2f}\"\n",
    "        cv2.putText(output_image, label, (x1, y1 - 10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, thickness)\n",
    "    \n",
    "    return output_image\n",
    "\n",
    "# Plot original OpenCV detections (green boxes)\n",
    "opencv_image = image.copy()\n",
    "for i in range(detections.shape[2]):\n",
    "    confidence = detections[0, 0, i, 2]\n",
    "    if confidence > 0.5:\n",
    "        box = detections[0, 0, i, 3:7] * np.array([image.shape[1], image.shape[0], image.shape[1], image.shape[0]])\n",
    "        (x1, y1, x2, y2) = box.astype(\"int\")\n",
    "        cv2.rectangle(opencv_image, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green\n",
    "\n",
    "# Plot YOLO-format detections (yellow boxes)\n",
    "yolo_image = yolo_to_bbox_and_plot(yolo_results, image)\n",
    "\n",
    "# Combine side-by-side for comparison\n",
    "combined_image = np.hstack([opencv_image, yolo_image])\n",
    "\n",
    "# Display\n",
    "cv2.imshow(\"Comparison: OpenCV (Green) vs YOLO (Yellow)\", combined_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combining all work together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load model\n",
    "prototxt_path = \"../ssic_image-corpus/scripts/benchmark/sfd_models/models/VGGNet/WIDER_FACE/SFD_trained/deploy.prototxt\"\n",
    "caffemodel_path = \"../ssic_image-corpus/scripts/benchmark/sfd_models/models/VGGNet/WIDER_FACE/SFD_trained/SFD.caffemodel\"\n",
    "net = cv2.dnn.readNetFromCaffe(prototxt_path, caffemodel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_yolo_labels(yolo_results, txt_output_path):\n",
    "\n",
    "    with open(txt_output_path, \"w\") as file:\n",
    "        for detection in yolo_results:\n",
    "            file.write(\" \".join([str(x) for x in detection]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runSingleFile(imagePath, outputPath, net):\n",
    "    # --- 2. Load and Preprocess Image ---\n",
    "    image = cv2.imread(imagePath)                     \n",
    "    (h, w) = image.shape[:2]                           \n",
    "    blob = cv2.dnn.blobFromImage(\n",
    "        image, \n",
    "        scalefactor=1.0,                \n",
    "        size=(640, 640),              \n",
    "        mean=(104.0, 177.0, 123.0),        \n",
    "        swapRB=False,                       \n",
    "        crop=False                          \n",
    "    )\n",
    "\n",
    "\n",
    "    net.setInput(blob)\n",
    "\n",
    "    yolo_results = convert_to_yolo_format(detections, image.shape[1], image.shape[0], class_id=0, outputPath=outputPath)\n",
    "\n",
    "    # optionally plotting\n",
    "\n",
    "    # opencv_image = image.copy()\n",
    "    # for i in range(detections.shape[2]):\n",
    "    #     confidence = detections[0, 0, i, 2]\n",
    "    #     if confidence > 0.5:\n",
    "    #         box = detections[0, 0, i, 3:7] * np.array([image.shape[1], image.shape[0], image.shape[1], image.shape[0]])\n",
    "    #         (x1, y1, x2, y2) = box.astype(\"int\")\n",
    "    #         cv2.rectangle(opencv_image, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green\n",
    "\n",
    "    # yolo_image = yolo_to_bbox_and_plot(yolo_results, image)\n",
    "\n",
    "    # combined_image = np.hstack([opencv_image, yolo_image])\n",
    "\n",
    "    # # Display\n",
    "    # cv2.imshow(\"Comparison: OpenCV (Green) vs YOLO (Yellow)\", combined_image)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the network (do this once globally to avoid reloading for each image)\n",
    "prototxt_path = \"../ssic_image-corpus/scripts/benchmark/sfd_models/models/VGGNet/WIDER_FACE/SFD_trained/deploy.prototxt\"\n",
    "caffemodel_path = \"../ssic_image-corpus/scripts/benchmark/sfd_models/models/VGGNet/WIDER_FACE/SFD_trained/SFD.caffemodel\"\n",
    "net = cv2.dnn.readNetFromCaffe(prototxt_path, caffemodel_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_yolo_format(detections, image_width, image_height, class_id=0, outputPath=None):\n",
    "\n",
    "    yolo_detections = []\n",
    "    \n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.5:  # Apply confidence threshold\n",
    "            # Get normalized box coordinates (x1, y1, x2, y2)\n",
    "            box = detections[0, 0, i, 3:7]\n",
    "            \n",
    "            # Convert to (x_center, y_center, width, height) and normalize\n",
    "            x_center = (box[0] + box[2]) / 2.0  # (x1 + x2)/2\n",
    "            y_center = (box[1] + box[3]) / 2.0  # (y1 + y2)/2\n",
    "            width = box[2] - box[0]             # x2 - x1\n",
    "            height = box[3] - box[1]            # y2 - y1\n",
    "            \n",
    "            # Append YOLO-format detection\n",
    "            yolo_detections.append([\n",
    "                class_id,\n",
    "                x_center,        \n",
    "                y_center,        \n",
    "                width,           \n",
    "                height,          \n",
    "                confidence      \n",
    "            ])\n",
    "    \n",
    "    # Save YOLO-format detections to file\n",
    "    # with open(outputPath, \"w\") as file:\n",
    "    #     for detection in yolo_detections:\n",
    "    #         file.write(\" \".join([str(x) for x in detection]) + \"\\n\")\n",
    "\n",
    "    return yolo_detections\n",
    "\n",
    "def save_yolo_labels(yolo_results, txt_output_path):\n",
    "\n",
    "    with open(txt_output_path, \"w\") as file:\n",
    "        for detection in yolo_results:\n",
    "            file.write(\" \".join([str(x) for x in detection]) + \"\\n\")\n",
    "\n",
    "def runSingleFile(imagePath, outputPath, net):\n",
    "    try:\n",
    "        image = cv2.imread(imagePath)\n",
    "        if image is None:\n",
    "            print(f\"Warning: Could not read image {imagePath}\")\n",
    "            return None\n",
    "            \n",
    "        (h, w) = image.shape[:2]\n",
    "        blob = cv2.dnn.blobFromImage(\n",
    "            image, \n",
    "            scalefactor=1.0,\n",
    "            size=(640, 640),\n",
    "            mean=(104.0, 177.0, 123.0),\n",
    "            swapRB=False,\n",
    "            crop=False\n",
    "        )\n",
    "\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "        \n",
    "        # Get base filename without extension\n",
    "        base_name = os.path.splitext(os.path.basename(imagePath))[0]\n",
    "        # txt_output_path = os.path.join(outputPath, f\"{base_name}.txt\")\n",
    "        \n",
    "        yolo_results = convert_to_yolo_format(detections, w, h)\n",
    "\n",
    "        print(outputPath)\n",
    "\n",
    "        save_yolo_labels(yolo_results, outputPath)\n",
    "        \n",
    "        return f\"Processed {imagePath} -> {outputPath}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error processing {imagePath}: {str(e)}\"\n",
    "\n",
    "def process_folder(input_folder, output_folder, num_processes=None):\n",
    "    allImage = os.listdir(input_folder)\n",
    "    # print(allImage)\n",
    "    \n",
    "    for x in allImage:\n",
    "        originalName, ext = os.path.splitext(x)\n",
    "        imagePath = os.path.join(input_folder, x)\n",
    "        outputPath = os.path.join(output_folder, originalName + \".txt\")\n",
    "        runSingleFile(imagePath, outputPath, net)\n",
    "        # print(f\"Processed {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"../ssic_image-corpus/data_phase-4_consensus/face/image_only/\"\n",
    "output_folder = \"../ssic_image-corpus/scripts/benchmark/s3fd_model_output\"\n",
    "process_folder(input_folder, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

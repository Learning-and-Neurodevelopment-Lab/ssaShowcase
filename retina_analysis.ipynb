{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retina Analysis\n",
    "This notebook contain the analysis for IoU for the retina model output. This goes into details on the analysis of missing instances for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_yolo_annotations(txt_path):\n",
    "    annotations = []\n",
    "    if os.path.exists(txt_path):\n",
    "        with open(txt_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                parts = list(map(float, line.strip().split()))\n",
    "                if len(parts) >= 6:\n",
    "                    annotations.append({\n",
    "                        'class': int(parts[0]),\n",
    "                        'bbox': parts[1:5],  # x_center, y_center, w, h\n",
    "                        'confidence': parts[5]\n",
    "                    })\n",
    "    return annotations\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    def to_corners(box):\n",
    "        x_center, y_center, w, h = box\n",
    "        x1 = x_center - w/2\n",
    "        y1 = y_center - h/2\n",
    "        x2 = x_center + w/2\n",
    "        y2 = y_center + h/2\n",
    "        return [x1, y1, x2, y2]\n",
    "    \n",
    "    box1 = to_corners(box1)\n",
    "    box2 = to_corners(box2)\n",
    "    \n",
    "    xi1 = max(box1[0], box2[0])\n",
    "    yi1 = max(box1[1], box2[1])\n",
    "    xi2 = min(box1[2], box2[2])\n",
    "    yi2 = min(box1[3], box2[3])\n",
    "    \n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "    box1_area = (box1[2]-box1[0])*(box1[3]-box1[1])\n",
    "    box2_area = (box2[2]-box2[0])*(box2[3]-box2[1])\n",
    "    \n",
    "    return inter_area / (box1_area + box2_area - inter_area + 1e-6)\n",
    "\n",
    "def evaluate_detection(gt_folder, pred_folder, iou_thresh=0.5):\n",
    "\n",
    "    all_preds = []\n",
    "    gt_db = defaultdict(list)\n",
    "    total_gts = 0\n",
    "\n",
    "    # Read all ground truths\n",
    "    for gt_file in os.listdir(gt_folder):\n",
    "        if not gt_file.endswith(\".txt\"): continue\n",
    "        base_name = os.path.splitext(gt_file)[0]\n",
    "        gt_path = os.path.join(gt_folder, gt_file)\n",
    "        gts = read_yolo_annotations(gt_path)\n",
    "        gt_db[base_name] = gts\n",
    "        total_gts += len(gts)\n",
    "\n",
    "    # Read all predictions and store with image ID\n",
    "    for pred_file in os.listdir(pred_folder):\n",
    "        if not pred_file.endswith(\".txt\"): continue\n",
    "        base_name = os.path.splitext(pred_file)[0]\n",
    "        pred_path = os.path.join(pred_folder, pred_file)\n",
    "        preds = read_yolo_annotations(pred_path)\n",
    "        \n",
    "        for pred in preds:\n",
    "            all_preds.append({\n",
    "                'image_id': base_name,\n",
    "                'confidence': pred['confidence'],\n",
    "                'bbox': pred['bbox']\n",
    "            })\n",
    "\n",
    "\n",
    "    all_preds.sort(key=lambda x: -x['confidence'])\n",
    "\n",
    "\n",
    "    matched_gts = defaultdict(set)  # Track matched GT indices per image\n",
    "    tp_list = []\n",
    "    fp_list = []\n",
    "    \n",
    "    for pred in all_preds:\n",
    "        image_id = pred['image_id']\n",
    "        pred_bbox = pred['bbox']\n",
    "        max_iou = 0.0\n",
    "        match_idx = -1\n",
    "\n",
    "        # Find best unmatched GT in this image\n",
    "        for idx, gt in enumerate(gt_db.get(image_id, [])):\n",
    "            if idx in matched_gts[image_id]: continue  # Skip already matched\n",
    "            iou = calculate_iou(pred_bbox, gt['bbox'])\n",
    "            if iou > max_iou:\n",
    "                max_iou = iou\n",
    "                match_idx = idx\n",
    "\n",
    "        # Determine TP/FP\n",
    "        if max_iou >= iou_thresh:\n",
    "            matched_gts[image_id].add(match_idx)\n",
    "            tp_list.append(1)\n",
    "            fp_list.append(0)\n",
    "        else:\n",
    "            tp_list.append(0)\n",
    "            fp_list.append(1)\n",
    "\n",
    "\n",
    "    tp = np.cumsum(tp_list)\n",
    "    fp = np.cumsum(fp_list)\n",
    "    fn = total_gts - tp\n",
    "    \n",
    "    precision = tp / (tp + fp + 1e-10)\n",
    "    recall = tp / (total_gts + 1e-10)\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'AP': average_precision_score(tp_list, [p['confidence'] for p in all_preds]),\n",
    "        'total_gts': total_gts,\n",
    "        'total_preds': len(all_preds),\n",
    "        'tp': tp[-1] if len(tp) > 0 else 0,\n",
    "        'fp': fp[-1] if len(fp) > 0 else 0,\n",
    "        'fn': fn[-1] if len(fn) > 0 else total_gts,\n",
    "        \"matched_gt\": matched_gts\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_metrics(results, iou_thresh=0.5):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    plt.subplot(121)\n",
    "    plt.plot(results['recall'], results['precision'], label=f'AP={results[\"AP\"]:.3f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve @ IoU={iou_thresh}')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Metrics Summary\n",
    "    plt.subplot(122)\n",
    "    metrics = [\n",
    "        ('True Positives', results['tp']),\n",
    "        ('False Positives', results['fp']),\n",
    "        ('False Negatives', results['fn']),\n",
    "        ('Total GT', results['total_gts']),\n",
    "        ('Total Pred', results['total_preds'])\n",
    "    ]\n",
    "    plt.barh([m[0] for m in metrics], [m[1] for m in metrics])\n",
    "    plt.title('Detection Breakdown')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder A (ground truth annotations) with Folder B (model predictions)\n",
    "# Run evaluation\n",
    "# groundTruth = \"../ssic_image-corpus/data_phase-4_consensus/face/label_face_only_actual_yolo\"\n",
    "\n",
    "groundTruth = \"../ssic_image-corpus/data_phase-4_consensus/face/label_face_only_actual_yolo\"\n",
    "\n",
    "modelPrediction = \"../ssic_image-corpus/data_phase-4_consensus/face/retinaface_output_toyolo\"\n",
    "\n",
    "\n",
    "results = evaluate_detection(groundTruth, modelPrediction, iou_thresh=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # return {\n",
    "    #     'precision': precision,\n",
    "    #     'recall': recall,\n",
    "    #     'AP': average_precision_score(tp_list, [p['confidence'] for p in all_preds]),\n",
    "    #     'total_gts': total_gts,\n",
    "    #     'total_preds': len(all_preds),\n",
    "    #     'tp': tp[-1] if len(tp) > 0 else 0,\n",
    "    #     'fp': fp[-1] if len(fp) > 0 else 0,\n",
    "    #     'fn': fn[-1] if len(fn) > 0 else total_gts,\n",
    "    #     \"matched_gt\": matched_gts\n",
    "    # }\n",
    "\n",
    "with open(\"../ssic_image-corpus/scripts/benchmark/retina_results_analysis_length.txt\", \"w\") as f:\n",
    "\n",
    "        f.write(f\"AP: {results['AP']:.3f}\\n\")\n",
    "        f.write(f\"Total GTs: {results['total_gts']}\\n\")\n",
    "        f.write(f\"Total Predictions: {results['total_preds']}\\n\")\n",
    "        f.write(f\"True Positives: {results['tp']}\\n\")\n",
    "        f.write(f\"False Positives: {results['fp']}\\n\")\n",
    "        f.write(f\"False Negatives: {results['fn']}\\n\")\n",
    "        f.write(f\"Matched GTs: {len(results['matched_gt'])}\\n\")\n",
    "        f.write(f\"Matched GTs per image:\\n\")\n",
    "        for image_id, matched in results['matched_gt'].items():\n",
    "            f.write(f\"{image_id}: {len(matched)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchedImage = results['matched_gt']\n",
    "\n",
    "with open(\"../ssic_image-corpus/scripts/benchmark/only_retina_results_analysis.txt\", \"w\") as f:\n",
    "    for image_id, matched in matchedImage.items():\n",
    "        f.write(f\"{image_id}: {matched}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this part focuses on the analysis of the model output\n",
    "`matchedImage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through each YOLO label file, go through each item, keep an index of the annotation, if the index doesn't match the number, record all attribute value in dictionary\n",
    "\n",
    "masterDict = {\"species\": {}, \"representation\": {}, \"raceethnicity\": {}, \"age\": {}, \"orientation\": {}, \"cameraangle\": {}, \"visibility\": {}, \"clarity\": {}, \"rotation\": {}}\n",
    "\n",
    "groundTruth = \"../ssic_image-corpus/data_phase-4_consensus/face/label_face_only_actual_labelme\"\n",
    "\n",
    "modelPrediction = \"../ssic_image-corpus/data_phase-4_consensus/face/retinaface_output_toyolo\"\n",
    "\n",
    "allGroundTruth = os.listdir(groundTruth)\n",
    "\n",
    "for x in allGroundTruth:\n",
    "    if not x.endswith(\".xml\"): \n",
    "        continue\n",
    "    base_name = os.path.splitext(x)[0]\n",
    "    gt_path = os.path.join(groundTruth, x)\n",
    "\n",
    "    # print(base_name)\n",
    "    # print(gt_path)\n",
    "\n",
    "    tree = ET.parse(gt_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    position = 0\n",
    "\n",
    "    # go through each object in the ground truth labelme file\n",
    "    for obj in list(root.findall(\"object\")):\n",
    "        nameElement = obj.find(\"name\")\n",
    "\n",
    "        # if no element, no face annotation\n",
    "        if nameElement is None:\n",
    "            \n",
    "            print(\"There is no annotation in this file\")\n",
    "            print(f\"File: {gt_path}, Object: {obj}\")\n",
    "\n",
    "        # if not face annotation, skip - WARNING\n",
    "        elif nameElement.text.strip().lower() != \"face\":\n",
    "            print(\"This is not a face annotation in this file\")\n",
    "            print(f\"File: {gt_path}, Object: {obj}\")\n",
    "        else:\n",
    "            \n",
    "            # if the position is in the matched image, then ignore\n",
    "            if position in matchedImage[base_name]:\n",
    "                position+= 1\n",
    "                continue\n",
    "\n",
    "            # if the position is not in the matched image, record the attribute count\n",
    "            attributes_element = obj.find('attributes')\n",
    "            if attributes_element is not None and attributes_element.text:\n",
    "                # Parse the species value\n",
    "                attributes = attributes_element.text\n",
    "\n",
    "                speciesPart, representationPart, raceethnicityPart, agePart, orientationPart, cameraanglePart, visibilityPart, clarityPart, rotationPart = attributes.split(\",\")\n",
    "\n",
    "                species = speciesPart.split(\"=\")[-1].strip()  # Extract the species value\n",
    "                representation = representationPart.split(\"=\")[-1].strip()\n",
    "                raceethnicity = raceethnicityPart.split(\"=\")[-1].strip()\n",
    "                age = agePart.split(\"=\")[-1].strip()\n",
    "                orientation = orientationPart.split(\"=\")[-1].strip()\n",
    "                cameraangle = cameraanglePart.split(\"=\")[-1].strip()\n",
    "                visibility = visibilityPart.split(\"=\")[-1].strip()\n",
    "                clarity = clarityPart.split(\"=\")[-1].strip()\n",
    "                rotation = rotationPart.split(\"=\")[-1].strip()\n",
    "            \n",
    "                # Add the values to the master dictionary\n",
    "                if species not in masterDict[\"species\"]:\n",
    "                    masterDict[\"species\"][species] = 1\n",
    "                else:\n",
    "                    masterDict[\"species\"][species] += 1\n",
    "                \n",
    "                if representation not in masterDict[\"representation\"]:\n",
    "                    masterDict[\"representation\"][representation] = 1\n",
    "                else:\n",
    "                    masterDict[\"representation\"][representation] += 1\n",
    "                \n",
    "                if raceethnicity not in masterDict[\"raceethnicity\"]:\n",
    "                    masterDict[\"raceethnicity\"][raceethnicity] = 1\n",
    "                else:\n",
    "                    masterDict[\"raceethnicity\"][raceethnicity] += 1\n",
    "                \n",
    "                if age not in masterDict[\"age\"]:\n",
    "                    masterDict[\"age\"][age] = 1\n",
    "                else:\n",
    "                    masterDict[\"age\"][age] += 1\n",
    "                \n",
    "                if orientation not in masterDict[\"orientation\"]:\n",
    "                    masterDict[\"orientation\"][orientation] = 1\n",
    "                else:\n",
    "                    masterDict[\"orientation\"][orientation] += 1\n",
    "                \n",
    "                if cameraangle not in masterDict[\"cameraangle\"]:\n",
    "                    masterDict[\"cameraangle\"][cameraangle] = 1\n",
    "                else:\n",
    "                    masterDict[\"cameraangle\"][cameraangle] += 1\n",
    "                \n",
    "                if visibility not in masterDict[\"visibility\"]:\n",
    "                    masterDict[\"visibility\"][visibility] = 1\n",
    "                else:\n",
    "                    masterDict[\"visibility\"][visibility] += 1\n",
    "                \n",
    "                if clarity not in masterDict[\"clarity\"]:\n",
    "                    masterDict[\"clarity\"][clarity] = 1\n",
    "                else:\n",
    "                    masterDict[\"clarity\"][clarity] += 1\n",
    "\n",
    "\n",
    "\n",
    "                position+= 1\n",
    "            else:\n",
    "                print(\"There is no attributes for this item\")\n",
    "                print(f\"File: {gt_path}, Object: {obj}\")\n",
    "                position+= 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../ssic_image-corpus/scripts/benchmark/retina_missing_breakdown.txt\", \"w\") as f:\n",
    "\n",
    "    for key, value in masterDict.items():\n",
    "        f.write(f\"{key}:\\n\")\n",
    "        for sub_key, sub_value in value.items():\n",
    "            f.write(f\"  {sub_key}: {sub_value}\\n\")\n",
    "        f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through each YOLO label file, , go through each item, keep an index of the annotation, if the index doesn't match the number, record all attribute value in dictionary\n",
    "\n",
    "completeMasterDict = {\"species\": {}, \"representation\": {}, \"raceethnicity\": {}, \"age\": {}, \"orientation\": {}, \"cameraangle\": {}, \"visibility\": {}, \"clarity\": {}, \"rotation\": {}}\n",
    "\n",
    "groundTruth = \"../ssic_image-corpus/data_phase-4_consensus/face/label_face_only_actual_labelme\"\n",
    "\n",
    "modelPrediction = \"../ssic_image-corpus/data_phase-4_consensus/face/retinaface_output_toyolo\"\n",
    "\n",
    "allGroundTruth = os.listdir(groundTruth)\n",
    "\n",
    "for x in allGroundTruth:\n",
    "    if not x.endswith(\".xml\"): \n",
    "        continue\n",
    "    base_name = os.path.splitext(x)[0]\n",
    "    gt_path = os.path.join(groundTruth, x)\n",
    "\n",
    "    # print(base_name)\n",
    "    # print(gt_path)\n",
    "\n",
    "    tree = ET.parse(gt_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    position = 0\n",
    "\n",
    "    # go through each object in the ground truth labelme file\n",
    "    for obj in list(root.findall(\"object\")):\n",
    "        nameElement = obj.find(\"name\")\n",
    "\n",
    "        # if no element, no face annotation\n",
    "        if nameElement is None:\n",
    "            \n",
    "            print(\"There is no annotation in this file\")\n",
    "            print(f\"File: {gt_path}, Object: {obj}\")\n",
    "\n",
    "        # if not face annotation, skip - WARNING\n",
    "        elif nameElement.text.strip().lower() != \"face\":\n",
    "            print(\"This is not a face annotation in this file\")\n",
    "            print(f\"File: {gt_path}, Object: {obj}\")\n",
    "        else:\n",
    "            \n",
    "            # if the position is in the matched image, then ignore\n",
    "            # if position in matchedImage[base_name]:\n",
    "            #     position+= 1\n",
    "            #     continue\n",
    "\n",
    "            # if the position is not in the matched image, record the attribute count\n",
    "            attributes_element = obj.find('attributes')\n",
    "            if attributes_element is not None and attributes_element.text:\n",
    "                # Parse the species value\n",
    "                attributes = attributes_element.text\n",
    "\n",
    "                speciesPart, representationPart, raceethnicityPart, agePart, orientationPart, cameraanglePart, visibilityPart, clarityPart, rotationPart = attributes.split(\",\")\n",
    "\n",
    "                species = speciesPart.split(\"=\")[-1].strip()  # Extract the species value\n",
    "                representation = representationPart.split(\"=\")[-1].strip()\n",
    "                raceethnicity = raceethnicityPart.split(\"=\")[-1].strip()\n",
    "                age = agePart.split(\"=\")[-1].strip()\n",
    "                orientation = orientationPart.split(\"=\")[-1].strip()\n",
    "                cameraangle = cameraanglePart.split(\"=\")[-1].strip()\n",
    "                visibility = visibilityPart.split(\"=\")[-1].strip()\n",
    "                clarity = clarityPart.split(\"=\")[-1].strip()\n",
    "                rotation = rotationPart.split(\"=\")[-1].strip()\n",
    "            \n",
    "                # Add the values to the master dictionary\n",
    "                if species not in completeMasterDict[\"species\"]:\n",
    "                    completeMasterDict[\"species\"][species] = 1\n",
    "                else:\n",
    "                    completeMasterDict[\"species\"][species] += 1\n",
    "                \n",
    "                if representation not in completeMasterDict[\"representation\"]:\n",
    "                    completeMasterDict[\"representation\"][representation] = 1\n",
    "                else:\n",
    "                    completeMasterDict[\"representation\"][representation] += 1\n",
    "                \n",
    "                if raceethnicity not in completeMasterDict[\"raceethnicity\"]:\n",
    "                    completeMasterDict[\"raceethnicity\"][raceethnicity] = 1\n",
    "                else:\n",
    "                    completeMasterDict[\"raceethnicity\"][raceethnicity] += 1\n",
    "                \n",
    "                if age not in completeMasterDict[\"age\"]:\n",
    "                    completeMasterDict[\"age\"][age] = 1\n",
    "                else:\n",
    "                    completeMasterDict[\"age\"][age] += 1\n",
    "                \n",
    "                if orientation not in completeMasterDict[\"orientation\"]:\n",
    "                    completeMasterDict[\"orientation\"][orientation] = 1\n",
    "                else:\n",
    "                    completeMasterDict[\"orientation\"][orientation] += 1\n",
    "                \n",
    "                if cameraangle not in completeMasterDict[\"cameraangle\"]:\n",
    "                    completeMasterDict[\"cameraangle\"][cameraangle] = 1\n",
    "                else:\n",
    "                    completeMasterDict[\"cameraangle\"][cameraangle] += 1\n",
    "                \n",
    "                if visibility not in completeMasterDict[\"visibility\"]:\n",
    "                    completeMasterDict[\"visibility\"][visibility] = 1\n",
    "                else:\n",
    "                    completeMasterDict[\"visibility\"][visibility] += 1\n",
    "                \n",
    "                if clarity not in completeMasterDict[\"clarity\"]:\n",
    "                    completeMasterDict[\"clarity\"][clarity] = 1\n",
    "                else:\n",
    "                    completeMasterDict[\"clarity\"][clarity] += 1\n",
    "\n",
    "\n",
    "\n",
    "                position+= 1\n",
    "            else:\n",
    "                print(\"There is no attributes for this item\")\n",
    "                print(f\"File: {gt_path}, Object: {obj}\")\n",
    "                position+= 1\n",
    "            \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../ssic_image-corpus/scripts/benchmark/retina_allInstance_breakdown.txt\", \"w\") as f:\n",
    "\n",
    "    for key, value in completeMasterDict.items():\n",
    "        f.write(f\"{key}:\\n\")\n",
    "        for sub_key, sub_value in value.items():\n",
    "            f.write(f\"  {sub_key}: {sub_value}\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingValues = []\n",
    "individualValue = 0\n",
    "completeValue = []\n",
    "\n",
    "print(\"Missing Instances by Attributes\")\n",
    "for key, value in masterDict.items():\n",
    "    print(key, value)\n",
    "    for sub_key, sub_value in value.items():\n",
    "        individualValue += sub_value\n",
    "    missingValues.append(individualValue)\n",
    "    individualValue = 0\n",
    "\n",
    "print(missingValues)\n",
    "        \n",
    "\n",
    "print(\"=======================\")\n",
    "\n",
    "print(\"All Instances by Attributes\")\n",
    "\n",
    "for key, value in completeMasterDict.items():\n",
    "    print(key, value)\n",
    "    for sub_key, sub_value in value.items():\n",
    "        individualValue += sub_value\n",
    "    completeValue.append(individualValue)\n",
    "    individualValue = 0\n",
    "\n",
    "print(completeValue)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeDict = {\"species\": {\"human\": [], \"puppet\": [], \"animal\": [], \"other\": []}, \n",
    "                \"representation\": {\"caricature\": [], \"real\": [], \"other\": []}, \n",
    "                \"raceethnicity\": {\"Black/African American\": [], \"other\": [], \"white\": [], \"Asian\": [], \"Native Hawaiian/Other Pacific Islander\": [], \"American Indian/Alaska Native\": []}, \n",
    "                \"age\": {\"child\": [], \"adult\": [], \"other\": [], \"infant\": [], \"teen\": [], \"elderly\": []}, \n",
    "                \"orientation\": {\"front-face\": [], \"side-profile\": [], \"other\": []}, \n",
    "                \"cameraangle\": {\"forward\": [], \"downward\": [], \"upward\": [], \"other\": []}, \n",
    "                \"visibility\": {\"full-view\": [], \"occluded\": [], \"truncated\": [], \"occluded-and-truncated\": [], \"other\": []}, \n",
    "                \"clarity\": {\"clear\": [], \"blurry\": [], \"other\": []}}\n",
    "\n",
    "with open(\"../ssic_image-corpus/scripts/benchmark/retina_missing_breakdown.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    current_key = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.endswith(\":\"):\n",
    "            current_key = line[:-1]\n",
    "            print(current_key)\n",
    "        elif current_key and \":\" in line:\n",
    "            sub_key, sub_value = line.split(\":\")\n",
    "            completeDict[current_key][sub_key.strip()].append(int(sub_value.strip()))\n",
    "    \n",
    "\n",
    "\n",
    "with open(\"../ssic_image-corpus/scripts/benchmark/retina_allInstance_breakdown.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    current_key = None\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.endswith(\":\"):\n",
    "            current_key = line[:-1]\n",
    "            print(current_key)\n",
    "        elif current_key and \":\" in line:\n",
    "            sub_key, sub_value = line.split(\":\")\n",
    "            completeDict[current_key][sub_key.strip()].append(int(sub_value.strip()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in completeDict.items():\n",
    "    for sub_key, sub_value in value.items():\n",
    "        sub_value.append(round(sub_value[0] / 5484, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in completeDict.items():\n",
    "    for sub_key, sub_value in value.items():\n",
    "        if len(sub_value) == 2:\n",
    "            completeDict[key][sub_key] = [0] + sub_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key , value in completeDict.items():\n",
    "    for sub_key, sub_value in value.items():\n",
    "        sub_value.append(round(sub_value[0] / sub_value[1], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "species human [622, 2259, 0.113, 0.275]\n",
      "species puppet [3404, 3492, 0.621, 0.975]\n",
      "species animal [727, 730, 0.133, 0.996]\n",
      "species other [731, 734, 0.133, 0.996]\n",
      "representation caricature [1983, 2027, 0.362, 0.978]\n",
      "representation real [3408, 5088, 0.621, 0.67]\n",
      "representation other [93, 100, 0.017, 0.93]\n",
      "raceethnicity Black/African American [77, 507, 0.014, 0.152]\n",
      "raceethnicity other [5239, 5802, 0.955, 0.903]\n",
      "raceethnicity white [154, 695, 0.028, 0.222]\n",
      "raceethnicity Asian [11, 199, 0.002, 0.055]\n",
      "raceethnicity Native Hawaiian/Other Pacific Islander [3, 3, 0.001, 1.0]\n",
      "raceethnicity American Indian/Alaska Native [0, 9, 0.002, 0.0]\n",
      "age child [242, 857, 0.044, 0.282]\n",
      "age adult [167, 1015, 0.03, 0.165]\n",
      "age other [5029, 5153, 0.917, 0.976]\n",
      "age infant [5, 15, 0.001, 0.333]\n",
      "age teen [22, 134, 0.004, 0.164]\n",
      "age elderly [19, 41, 0.003, 0.463]\n",
      "orientation front-face [1508, 1996, 0.275, 0.756]\n",
      "orientation side-profile [431, 567, 0.079, 0.76]\n",
      "orientation other [3545, 4652, 0.646, 0.762]\n",
      "cameraangle forward [5151, 6749, 0.939, 0.763]\n",
      "cameraangle downward [231, 318, 0.042, 0.726]\n",
      "cameraangle upward [94, 140, 0.017, 0.671]\n",
      "cameraangle other [8, 8, 0.001, 1.0]\n",
      "visibility full-view [4548, 5866, 0.829, 0.775]\n",
      "visibility occluded [823, 1208, 0.15, 0.681]\n",
      "visibility truncated [85, 105, 0.015, 0.81]\n",
      "visibility occluded-and-truncated [18, 26, 0.003, 0.692]\n",
      "visibility other [10, 10, 0.002, 1.0]\n",
      "clarity clear [4965, 6371, 0.905, 0.779]\n",
      "clarity blurry [461, 781, 0.084, 0.59]\n",
      "clarity other [58, 63, 0.011, 0.921]\n"
     ]
    }
   ],
   "source": [
    "for key, value in completeDict.items():\n",
    "    for sub_key, sub_value in value.items():\n",
    "        print(key, sub_key, sub_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the final values used and kept track of\n",
    "The variable is `completeDicy`, and it contains the following values: \\\n",
    "missing instances, all instances, missing instances / all missing instances, missing instance / all instances of this attribute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "species human [622, 2259, 0.113, 0.275]\n",
      "species puppet [3404, 3492, 0.621, 0.975]\n",
      "species animal [727, 730, 0.133, 0.996]\n",
      "species other [731, 734, 0.133, 0.996]\n",
      "representation caricature [1983, 2027, 0.362, 0.978]\n",
      "representation real [3408, 5088, 0.621, 0.67]\n",
      "representation other [93, 100, 0.017, 0.93]\n",
      "raceethnicity Black/African American [77, 507, 0.014, 0.152]\n",
      "raceethnicity other [5239, 5802, 0.955, 0.903]\n",
      "raceethnicity white [154, 695, 0.028, 0.222]\n",
      "raceethnicity Asian [11, 199, 0.002, 0.055]\n",
      "raceethnicity Native Hawaiian/Other Pacific Islander [3, 3, 0.001, 1.0]\n",
      "raceethnicity American Indian/Alaska Native [0, 9, 0.002, 0.0]\n",
      "age child [242, 857, 0.044, 0.282]\n",
      "age adult [167, 1015, 0.03, 0.165]\n",
      "age other [5029, 5153, 0.917, 0.976]\n",
      "age infant [5, 15, 0.001, 0.333]\n",
      "age teen [22, 134, 0.004, 0.164]\n",
      "age elderly [19, 41, 0.003, 0.463]\n",
      "orientation front-face [1508, 1996, 0.275, 0.756]\n",
      "orientation side-profile [431, 567, 0.079, 0.76]\n",
      "orientation other [3545, 4652, 0.646, 0.762]\n",
      "cameraangle forward [5151, 6749, 0.939, 0.763]\n",
      "cameraangle downward [231, 318, 0.042, 0.726]\n",
      "cameraangle upward [94, 140, 0.017, 0.671]\n",
      "cameraangle other [8, 8, 0.001, 1.0]\n",
      "visibility full-view [4548, 5866, 0.829, 0.775]\n",
      "visibility occluded [823, 1208, 0.15, 0.681]\n",
      "visibility truncated [85, 105, 0.015, 0.81]\n",
      "visibility occluded-and-truncated [18, 26, 0.003, 0.692]\n",
      "visibility other [10, 10, 0.002, 1.0]\n",
      "clarity clear [4965, 6371, 0.905, 0.779]\n",
      "clarity blurry [461, 781, 0.084, 0.59]\n",
      "clarity other [58, 63, 0.011, 0.921]\n"
     ]
    }
   ],
   "source": [
    "for key, value in completeDict.items():\n",
    "    for sub_key, sub_value in value.items():\n",
    "        print(key, sub_key, sub_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the \"others\" category\n",
    "\n",
    "graphDict = completeDict\n",
    "\n",
    "for key, value in graphDict.items():\n",
    "    for sub_key in list(value.keys()):\n",
    "        if sub_key == \"other\":\n",
    "            del graphDict[key][sub_key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "species human [622, 2259, 0.113, 0.275]\n",
      "species puppet [3404, 3492, 0.621, 0.975]\n",
      "species animal [727, 730, 0.133, 0.996]\n",
      "representation caricature [1983, 2027, 0.362, 0.978]\n",
      "representation real [3408, 5088, 0.621, 0.67]\n",
      "raceethnicity Black/African American [77, 507, 0.014, 0.152]\n",
      "raceethnicity white [154, 695, 0.028, 0.222]\n",
      "raceethnicity Asian [11, 199, 0.002, 0.055]\n",
      "raceethnicity Native Hawaiian/Other Pacific Islander [3, 3, 0.001, 1.0]\n",
      "raceethnicity American Indian/Alaska Native [0, 9, 0.002, 0.0]\n",
      "age child [242, 857, 0.044, 0.282]\n",
      "age adult [167, 1015, 0.03, 0.165]\n",
      "age infant [5, 15, 0.001, 0.333]\n",
      "age teen [22, 134, 0.004, 0.164]\n",
      "age elderly [19, 41, 0.003, 0.463]\n",
      "orientation front-face [1508, 1996, 0.275, 0.756]\n",
      "orientation side-profile [431, 567, 0.079, 0.76]\n",
      "cameraangle forward [5151, 6749, 0.939, 0.763]\n",
      "cameraangle downward [231, 318, 0.042, 0.726]\n",
      "cameraangle upward [94, 140, 0.017, 0.671]\n",
      "visibility full-view [4548, 5866, 0.829, 0.775]\n",
      "visibility occluded [823, 1208, 0.15, 0.681]\n",
      "visibility truncated [85, 105, 0.015, 0.81]\n",
      "visibility occluded-and-truncated [18, 26, 0.003, 0.692]\n",
      "clarity clear [4965, 6371, 0.905, 0.779]\n",
      "clarity blurry [461, 781, 0.084, 0.59]\n"
     ]
    }
   ],
   "source": [
    "for key, value in graphDict.items():\n",
    "    for sub_key, sub_value in value.items():\n",
    "        print(key, sub_key, sub_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_graphDict = {}\n",
    "\n",
    "for key, value in graphDict.items():\n",
    "    # Sort sub-dictionary by the 4th element (index 3) in descending order\n",
    "    sorted_items = dict(sorted(value.items(), key=lambda item: item[1][3], reverse=False))\n",
    "    sorted_graphDict[key] = sorted_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted graphDict:\n",
      "species animal [727, 730, 0.133, 0.996]\n",
      "species puppet [3404, 3492, 0.621, 0.975]\n",
      "species human [622, 2259, 0.113, 0.275]\n",
      "representation caricature [1983, 2027, 0.362, 0.978]\n",
      "representation real [3408, 5088, 0.621, 0.67]\n",
      "raceethnicity Native Hawaiian/Other Pacific Islander [3, 3, 0.001, 1.0]\n",
      "raceethnicity white [154, 695, 0.028, 0.222]\n",
      "raceethnicity Black/African American [77, 507, 0.014, 0.152]\n",
      "raceethnicity Asian [11, 199, 0.002, 0.055]\n",
      "raceethnicity American Indian/Alaska Native [0, 9, 0.002, 0.0]\n",
      "age elderly [19, 41, 0.003, 0.463]\n",
      "age infant [5, 15, 0.001, 0.333]\n",
      "age child [242, 857, 0.044, 0.282]\n",
      "age adult [167, 1015, 0.03, 0.165]\n",
      "age teen [22, 134, 0.004, 0.164]\n",
      "orientation side-profile [431, 567, 0.079, 0.76]\n",
      "orientation front-face [1508, 1996, 0.275, 0.756]\n",
      "cameraangle forward [5151, 6749, 0.939, 0.763]\n",
      "cameraangle downward [231, 318, 0.042, 0.726]\n",
      "cameraangle upward [94, 140, 0.017, 0.671]\n",
      "visibility truncated [85, 105, 0.015, 0.81]\n",
      "visibility full-view [4548, 5866, 0.829, 0.775]\n",
      "visibility occluded-and-truncated [18, 26, 0.003, 0.692]\n",
      "visibility occluded [823, 1208, 0.15, 0.681]\n",
      "clarity clear [4965, 6371, 0.905, 0.779]\n",
      "clarity blurry [461, 781, 0.084, 0.59]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sorted graphDict:\")\n",
    "for key, value in sorted_graphDict.items():\n",
    "    for sub_key, sub_value in value.items():\n",
    "        print(key, sub_key, sub_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in sorted_graphDict.items():\n",
    "    for sub_key in list(value.keys()): \n",
    "        new_key = sub_key.title()\n",
    "        sorted_graphDict[key][new_key] = sorted_graphDict[key].pop(sub_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "species Animal [727, 730, 0.133, 0.996]\n",
      "species Puppet [3404, 3492, 0.621, 0.975]\n",
      "species Human [622, 2259, 0.113, 0.275]\n",
      "representation Caricature [1983, 2027, 0.362, 0.978]\n",
      "representation Real [3408, 5088, 0.621, 0.67]\n",
      "raceethnicity Native Hawaiian/Other Pacific Islander [3, 3, 0.001, 1.0]\n",
      "raceethnicity White [154, 695, 0.028, 0.222]\n",
      "raceethnicity Black/African American [77, 507, 0.014, 0.152]\n",
      "raceethnicity Asian [11, 199, 0.002, 0.055]\n",
      "raceethnicity American Indian/Alaska Native [0, 9, 0.002, 0.0]\n",
      "age Elderly [19, 41, 0.003, 0.463]\n",
      "age Infant [5, 15, 0.001, 0.333]\n",
      "age Child [242, 857, 0.044, 0.282]\n",
      "age Adult [167, 1015, 0.03, 0.165]\n",
      "age Teen [22, 134, 0.004, 0.164]\n",
      "orientation Side-Profile [431, 567, 0.079, 0.76]\n",
      "orientation Front-Face [1508, 1996, 0.275, 0.756]\n",
      "cameraangle Forward [5151, 6749, 0.939, 0.763]\n",
      "cameraangle Downward [231, 318, 0.042, 0.726]\n",
      "cameraangle Upward [94, 140, 0.017, 0.671]\n",
      "visibility Truncated [85, 105, 0.015, 0.81]\n",
      "visibility Full-View [4548, 5866, 0.829, 0.775]\n",
      "visibility Occluded-And-Truncated [18, 26, 0.003, 0.692]\n",
      "visibility Occluded [823, 1208, 0.15, 0.681]\n",
      "clarity Clear [4965, 6371, 0.905, 0.779]\n",
      "clarity Blurry [461, 781, 0.084, 0.59]\n"
     ]
    }
   ],
   "source": [
    "for key, value in sorted_graphDict.items():\n",
    "    for sub_key, sub_value in value.items():\n",
    "        print(key, sub_key, sub_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining native hawaiian and pacific islander and american indian and alaskan native\n",
    "\n",
    "sorted_graphDict[\"raceethnicity\"][\"Indigenous Peoples\"] = [3, 12, 0.001, 0.25]\n",
    "del sorted_graphDict[\"raceethnicity\"][\"Native Hawaiian/Other Pacific Islander\"]\n",
    "del sorted_graphDict[\"raceethnicity\"][\"American Indian/Alaska Native\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "species Animal [727, 730, 0.133, 0.996]\n",
      "species Puppet [3404, 3492, 0.621, 0.975]\n",
      "species Human [622, 2259, 0.113, 0.275]\n",
      "representation Caricature [1983, 2027, 0.362, 0.978]\n",
      "representation Real [3408, 5088, 0.621, 0.67]\n",
      "raceethnicity White [154, 695, 0.028, 0.222]\n",
      "raceethnicity Black/African American [77, 507, 0.014, 0.152]\n",
      "raceethnicity Asian [11, 199, 0.002, 0.055]\n",
      "raceethnicity Indigenous Peoples [3, 12, 0.001, 0.25]\n",
      "age Elderly [19, 41, 0.003, 0.463]\n",
      "age Infant [5, 15, 0.001, 0.333]\n",
      "age Child [242, 857, 0.044, 0.282]\n",
      "age Adult [167, 1015, 0.03, 0.165]\n",
      "age Teen [22, 134, 0.004, 0.164]\n",
      "orientation Side-Profile [431, 567, 0.079, 0.76]\n",
      "orientation Front-Face [1508, 1996, 0.275, 0.756]\n",
      "cameraangle Forward [5151, 6749, 0.939, 0.763]\n",
      "cameraangle Downward [231, 318, 0.042, 0.726]\n",
      "cameraangle Upward [94, 140, 0.017, 0.671]\n",
      "visibility Truncated [85, 105, 0.015, 0.81]\n",
      "visibility Full-View [4548, 5866, 0.829, 0.775]\n",
      "visibility Occluded-And-Truncated [18, 26, 0.003, 0.692]\n",
      "visibility Occluded [823, 1208, 0.15, 0.681]\n",
      "clarity Clear [4965, 6371, 0.905, 0.779]\n",
      "clarity Blurry [461, 781, 0.084, 0.59]\n"
     ]
    }
   ],
   "source": [
    "for key, value in sorted_graphDict.items():\n",
    "    for sub_key, sub_value in value.items():\n",
    "        print(key, sub_key, sub_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalSortedDict = {}\n",
    "\n",
    "for key, value in sorted_graphDict.items():\n",
    "    # Sort sub-dictionary by the 4th element (index 3) in descending order\n",
    "    sorted_items = dict(sorted(value.items(), key=lambda item: item[1][3], reverse=True))\n",
    "    finalSortedDict[key] = sorted_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "species Animal [727, 730, 0.133, 0.996]\n",
      "species Puppet [3404, 3492, 0.621, 0.975]\n",
      "species Human [622, 2259, 0.113, 0.275]\n",
      "representation Caricature [1983, 2027, 0.362, 0.978]\n",
      "representation Real [3408, 5088, 0.621, 0.67]\n",
      "raceethnicity Indigenous Peoples [3, 12, 0.001, 0.25]\n",
      "raceethnicity White [154, 695, 0.028, 0.222]\n",
      "raceethnicity Black/African American [77, 507, 0.014, 0.152]\n",
      "raceethnicity Asian [11, 199, 0.002, 0.055]\n",
      "age Elderly [19, 41, 0.003, 0.463]\n",
      "age Infant [5, 15, 0.001, 0.333]\n",
      "age Child [242, 857, 0.044, 0.282]\n",
      "age Adult [167, 1015, 0.03, 0.165]\n",
      "age Teen [22, 134, 0.004, 0.164]\n",
      "orientation Side-Profile [431, 567, 0.079, 0.76]\n",
      "orientation Front-Face [1508, 1996, 0.275, 0.756]\n",
      "cameraangle Forward [5151, 6749, 0.939, 0.763]\n",
      "cameraangle Downward [231, 318, 0.042, 0.726]\n",
      "cameraangle Upward [94, 140, 0.017, 0.671]\n",
      "visibility Truncated [85, 105, 0.015, 0.81]\n",
      "visibility Full-View [4548, 5866, 0.829, 0.775]\n",
      "visibility Occluded-And-Truncated [18, 26, 0.003, 0.692]\n",
      "visibility Occluded [823, 1208, 0.15, 0.681]\n",
      "clarity Clear [4965, 6371, 0.905, 0.779]\n",
      "clarity Blurry [461, 781, 0.084, 0.59]\n"
     ]
    }
   ],
   "source": [
    "for key, value in finalSortedDict.items():\n",
    "    for sub_key, sub_value in value.items():\n",
    "        print(key, sub_key, sub_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Example color mapping for each major category\n",
    "category_colors = {\n",
    "    'species': '#FF1236',\n",
    "    'representation': '#F17DBA',\n",
    "    'raceethnicity': '#FFAA01',\n",
    "    'age': '#53B810',\n",
    "    'orientation': '#02713E',\n",
    "    'cameraangle': '#0070BC',\n",
    "    'visibility': '#02306B',\n",
    "    'clarity': '#4F3692',\n",
    "}\n",
    "\n",
    "graphDict = finalSortedDict\n",
    "\n",
    "# Extract and prepare data\n",
    "labels = []\n",
    "count_c = []\n",
    "count_rest = []\n",
    "bar_colors = []\n",
    "\n",
    "for category, subcategories in graphDict.items():\n",
    "    base_color = category_colors.get(category, 'steelblue')\n",
    "    for subcat, val in subcategories.items():\n",
    "        labels.append(subcat)\n",
    "        c = val[3]\n",
    "        count_c.append(c)\n",
    "        count_rest.append(1 - c)\n",
    "        bar_colors.append(base_color)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 0.3 * len(labels)))\n",
    "y_pos = range(len(labels))\n",
    "\n",
    "# Stacked bars\n",
    "bars1 = ax.barh(y_pos, count_rest, color=\"lightgray\", label='Proportion of Recognized Instances',)\n",
    "bars2 = ax.barh(y_pos, count_c, left=count_rest, color=bar_colors, label='Proportion of Missed Instances')\n",
    "\n",
    "# Labels\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(labels, fontsize = 12)\n",
    "ax.invert_yaxis()\n",
    "ax.tick_params(axis='x', labelsize=10)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_xlabel(\"Proportion\", fontsize = 12)\n",
    "# ax.set_title(\"Proportion of Recognized vs Missed Instances for Each Attribute Value\", fontsize = 20)\n",
    "# Build legend handles for each category color\n",
    "legend_handles = []\n",
    "for category, color in category_colors.items():\n",
    "    patch = Patch(facecolor=color, label=category.capitalize())\n",
    "    legend_handles.append(patch)\n",
    "\n",
    "# Add custom legend\n",
    "ax.legend(\n",
    "    handles=legend_handles,\n",
    "    loc='lower right',\n",
    "    bbox_to_anchor=(0.9, -0.2),  # adjust as needed\n",
    "    fontsize=9,\n",
    "    title=\"Attribute Category\",\n",
    "    title_fontsize=10,\n",
    "    ncol=4,\n",
    ")\n",
    "# Annotate each section\n",
    "# Get category list for each bar\n",
    "\n",
    "bar_categories = []\n",
    "for category, subcategories in graphDict.items():\n",
    "    for _ in subcategories:\n",
    "        bar_categories.append(category)\n",
    "\n",
    "# Annotate each section with conditional color\n",
    "for i, (c, r) in enumerate(zip(count_c, count_rest)):\n",
    "    category = bar_categories[i]\n",
    "    is_dark = category in ['orientation', 'cameraangle', 'visibility', 'clarity']\n",
    "    text_color = 'white' if is_dark else 'black'\n",
    "\n",
    "    # Annotate count_rest directly if big enough\n",
    "    if r > 0.05:\n",
    "        ax.text(r / 2, i, f\"{r:.3f}\", ha='center', va='center', fontsize=10, color='black')\n",
    "    else:\n",
    "        ax.annotate(f\"{r:.3f}\",\n",
    "                    xy=(max(0, r - 0.01), i), \n",
    "                    xytext=(r + 0.01, i - 0.3),  # shift label outward\n",
    "                    textcoords='data',\n",
    "                    fontsize=10,\n",
    "                    color='black',\n",
    "                    arrowprops=dict(arrowstyle='-', color='black', lw=2),\n",
    "                    ha='left', va='center')\n",
    "\n",
    "    # Annotate count_c directly if big enough\n",
    "    if c > 0.05:\n",
    "        ax.text(r + c / 2, i, f\"{c:.3f}\", ha='center', va='center', fontsize=10, color=text_color)\n",
    "    else:\n",
    "        ax.annotate(f\"{c:.3f}\",\n",
    "                    xy=(r + c, i),\n",
    "                    xytext=(r + c + 0.02, i + 0.3),\n",
    "                    textcoords='data',\n",
    "                    fontsize=10,\n",
    "                    color=text_color,\n",
    "                    arrowprops=dict(arrowstyle='-', color='gray', lw=1),\n",
    "                    ha='left', va='center')\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
